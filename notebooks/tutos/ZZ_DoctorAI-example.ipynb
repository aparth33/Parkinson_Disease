{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes MIMIC-III dataset and builds longitudinal diagnosis records for patients with at least two visits.\n",
    "# The output data are cPickled, and suitable for training Doctor AI or RETAIN\n",
    "# Written by Edward Choi (mp2893@gatech.edu)\n",
    "# Usage: Put this script to the foler where MIMIC-III CSV files are located. Then execute the below command.\n",
    "# python process_mimic.py ADMISSIONS.csv DIAGNOSES_ICD.csv <output file> \n",
    "\n",
    "# Output files\n",
    "# <output file>.pids: List of unique Patient IDs. Used for intermediate processing\n",
    "# <output file>.dates: List of List of Python datetime objects. The outer List is for each patient. The inner List is for each visit made by each patient\n",
    "# <output file>.seqs: List of List of List of integer diagnosis codes. The outer List is for each patient. The middle List contains visits made by each patient. The inner List contains the integer diagnosis codes that occurred in each visit\n",
    "# <output file>.types: Python dictionary that maps string diagnosis codes to integer diagnosis codes.\n",
    "\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# conversion of medical codes/ \n",
    "def convert_to_icd9(dxStr):\n",
    "\tif dxStr.startswith('E'):\n",
    "\t\tif len(dxStr) > 4: return dxStr[:4] + '.' + dxStr[4:]\n",
    "\t\telse: return dxStr\n",
    "\telse:\n",
    "\t\tif len(dxStr) > 3: return dxStr[:3] + '.' + dxStr[3:]\n",
    "\t\telse: return dxStr\n",
    "\t\n",
    "def convert_to_3digit_icd9(dxStr):\n",
    "\tif dxStr.startswith('E'):\n",
    "\t\tif len(dxStr) > 4: return dxStr[:4]\n",
    "\t\telse: return dxStr\n",
    "\telse:\n",
    "\t\tif len(dxStr) > 3: return dxStr[:3]\n",
    "\t\telse: return dxStr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.quora.com/What-is-sys-argv-in-python-and-how-is-it-used        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tadmissionFile = sys.argv[1]\n",
    "\tdiagnosisFile = sys.argv[2]\n",
    "\toutFile = sys.argv[3]\n",
    "\n",
    "\tprint 'Building pid-admission mapping, admission-date mapping'\n",
    "\tpidAdmMap = {}\n",
    "\tadmDateMap = {}\n",
    "\tinfd = open(admissionFile, 'r')\n",
    "\tinfd.readline()\n",
    "\tfor line in infd:\n",
    "\t\ttokens = line.strip().split(',')\n",
    "\t\tpid = int(tokens[1])\n",
    "\t\tadmId = int(tokens[2])\n",
    "\t\tadmTime = datetime.strptime(tokens[3], '%Y-%m-%d %H:%M:%S')\n",
    "\t\tadmDateMap[admId] = admTime\n",
    "\t\tif pid in pidAdmMap: pidAdmMap[pid].append(admId)\n",
    "\t\telse: pidAdmMap[pid] = [admId]\n",
    "\tinfd.close()\n",
    "\n",
    "\tprint 'Building admission-dxList mapping'\n",
    "\tadmDxMap = {}\n",
    "\tinfd = open(diagnosisFile, 'r')\n",
    "\tinfd.readline()\n",
    "\tfor line in infd:\n",
    "\t\ttokens = line.strip().split(',')\n",
    "\t\tadmId = int(tokens[2])\n",
    "\t\tdxStr = 'D_' + convert_to_icd9(tokens[4][1:-1]) ############## Uncomment this line and comment the line below, if you want to use the entire ICD9 digits.\n",
    "\t\t#dxStr = 'D_' + convert_to_3digit_icd9(tokens[4][1:-1])\n",
    "\t\tif admId in admDxMap: admDxMap[admId].append(dxStr)\n",
    "\t\telse: admDxMap[admId] = [dxStr]\n",
    "\tinfd.close()\n",
    "\n",
    "\tprint 'Building pid-sortedVisits mapping'\n",
    "\tpidSeqMap = {}\n",
    "\tfor pid, admIdList in pidAdmMap.iteritems():\n",
    "\t\tif len(admIdList) < 2: continue\n",
    "\t\tsortedList = sorted([(admDateMap[admId], admDxMap[admId]) for admId in admIdList])\n",
    "\t\tpidSeqMap[pid] = sortedList\n",
    "\t\n",
    "\tprint 'Building pids, dates, strSeqs'\n",
    "\tpids = []\n",
    "\tdates = []\n",
    "\tseqs = []\n",
    "\tfor pid, visits in pidSeqMap.iteritems():\n",
    "\t\tpids.append(pid)\n",
    "\t\tseq = []\n",
    "\t\tdate = []\n",
    "\t\tfor visit in visits:\n",
    "\t\t\tdate.append(visit[0])\n",
    "\t\t\tseq.append(visit[1])\n",
    "\t\tdates.append(date)\n",
    "\t\tseqs.append(seq)\n",
    "\t\n",
    "\tprint 'Converting strSeqs to intSeqs, and making types'\n",
    "\ttypes = {}\n",
    "\tnewSeqs = []\n",
    "\tfor patient in seqs:\n",
    "\t\tnewPatient = []\n",
    "\t\tfor visit in patient:\n",
    "\t\t\tnewVisit = []\n",
    "\t\t\tfor code in visit:\n",
    "\t\t\t\tif code in types:\n",
    "\t\t\t\t\tnewVisit.append(types[code])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttypes[code] = len(types)\n",
    "\t\t\t\t\tnewVisit.append(types[code])\n",
    "\t\t\tnewPatient.append(newVisit)\n",
    "\t\tnewSeqs.append(newPatient)\n",
    "\n",
    "\tpickle.dump(pids, open(outFile+'.pids', 'wb'), -1)\n",
    "\tpickle.dump(dates, open(outFile+'.dates', 'wb'), -1)\n",
    "\tpickle.dump(newSeqs, open(outFile+'.seqs', 'wb'), -1)\n",
    "\tpickle.dump(types, open(outFile+'.types', 'wb'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zipped):\n",
    "\tnew_params = OrderedDict()\n",
    "\tfor key, value in zipped.iteritems():\n",
    "\t\tnew_params[key] = value.get_value()\n",
    "\treturn new_params\n",
    "\n",
    "def numpy_floatX(data):\n",
    "\treturn np.asarray(data, dtype=config.floatX)\n",
    "\n",
    "def load_embedding(infile):\n",
    "\tWemb = np.array(pickle.load(open(infile, 'rb'))).astype(config.floatX)\n",
    "\treturn Wemb\n",
    "\n",
    "def init_params(options):\n",
    "\tparams = OrderedDict()\n",
    "\ttimeFile = options['timeFile']\n",
    "\tembFile = options['embFile']\n",
    "\tembSize = options['embSize']\n",
    "\tinputDimSize = options['inputDimSize']\n",
    "\tnumClass = options['numClass']\n",
    "\n",
    "\tif len(embFile) > 0: \n",
    "\t\tprint 'using external code embedding'\n",
    "\t\tparams['W_emb'] = load_embedding(embFile)\n",
    "\t\tembSize = params['W_emb'].shape[1]\n",
    "\telse: \n",
    "\t\tprint 'using randomly initialized code embedding'\n",
    "\t\tparams['W_emb'] = np.random.uniform(-0.01, 0.01, (inputDimSize, embSize)).astype(config.floatX)\n",
    "\tparams['b_emb'] = np.zeros(embSize).astype(config.floatX)\n",
    "\n",
    "\tprevDimSize = embSize\n",
    "\tif len(timeFile) > 0: prevDimSize += 1 #We need to consider an extra dimension for the duration information\n",
    "\tfor count, hiddenDimSize in enumerate(options['hiddenDimSize']):\n",
    "\t\tparams['W_'+str(count)] = np.random.uniform(-0.01, 0.01, (prevDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['W_r_'+str(count)] = np.random.uniform(-0.01, 0.01, (prevDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['W_z_'+str(count)] = np.random.uniform(-0.01, 0.01, (prevDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['U_'+str(count)] = np.random.uniform(-0.01, 0.01, (hiddenDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['U_r_'+str(count)] = np.random.uniform(-0.01, 0.01, (hiddenDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['U_z_'+str(count)] = np.random.uniform(-0.01, 0.01, (hiddenDimSize, hiddenDimSize)).astype(config.floatX)\n",
    "\t\tparams['b_'+str(count)] = np.zeros(hiddenDimSize).astype(config.floatX)\n",
    "\t\tparams['b_r_'+str(count)] = np.zeros(hiddenDimSize).astype(config.floatX)\n",
    "\t\tparams['b_z_'+str(count)] = np.zeros(hiddenDimSize).astype(config.floatX)\n",
    "\t\tprevDimSize = hiddenDimSize\n",
    "\n",
    "\tparams['W_output'] = np.random.uniform(-0.01, 0.01, (prevDimSize, numClass)).astype(config.floatX)\n",
    "\tparams['b_output'] = np.zeros(numClass).astype(config.floatX)\n",
    "\n",
    "\tif options['predictTime']:\n",
    "\t\tparams['W_time'] = np.random.uniform(-0.01, 0.01, (prevDimSize, 1)).astype(config.floatX)\n",
    "\t\tparams['b_time'] = np.zeros(1).astype(config.floatX)\n",
    "\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tparams(params, options):\n",
    "\ttparams = OrderedDict()\n",
    "\tfor key, value in params.iteritems():\n",
    "\t\tif not options['embFineTune'] and key == 'W_emb': continue\n",
    "\t\ttparams[key] = theano.shared(value, name=key)\n",
    "\treturn tparams\n",
    "\n",
    "def dropout_layer(state_before, use_noise, trng, dropout_rate):\n",
    "\tproj = T.switch(use_noise, (state_before * trng.binomial(state_before.shape, p=dropout_rate, n=1, dtype=state_before.dtype)), state_before * 0.5)\n",
    "\treturn proj\n",
    "\n",
    "def gru_layer(tparams, emb, layerIndex, hiddenDimSize, mask=None):\n",
    "\ttimesteps = emb.shape[0]\n",
    "\tif emb.ndim == 3: n_samples = emb.shape[1]\n",
    "\telse: n_samples = 1\n",
    "\n",
    "\tW_rx = T.dot(emb, tparams['W_r_'+layerIndex])\n",
    "\tW_zx = T.dot(emb, tparams['W_z_'+layerIndex])\n",
    "\tWx = T.dot(emb, tparams['W_'+layerIndex])\n",
    "\n",
    "\tdef stepFn(stepMask, wrx, wzx, wx, h):\n",
    "\t\tr = T.nnet.sigmoid(wrx + T.dot(h, tparams['U_r_'+layerIndex]) + tparams['b_r_'+layerIndex])\n",
    "\t\tz = T.nnet.sigmoid(wzx + T.dot(h, tparams['U_z_'+layerIndex]) + tparams['b_z_'+layerIndex])\n",
    "\t\th_tilde = T.tanh(wx + T.dot(r*h, tparams['U_'+layerIndex]) + tparams['b_'+layerIndex])\n",
    "\t\th_new = z * h + ((1. - z) * h_tilde)\n",
    "\t\th_new = stepMask[:, None] * h_new + (1. - stepMask)[:, None] * h\n",
    "\t\treturn h_new\n",
    "\n",
    "\tresults, updates = theano.scan(fn=stepFn, sequences=[mask,W_rx,W_zx,Wx], outputs_info=T.alloc(numpy_floatX(0.0), n_samples, hiddenDimSize), name='gru_layer'+layerIndex, n_steps=timesteps)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "def build_model(tparams, options, W_emb=None):\n",
    "\ttrng = RandomStreams(123)\n",
    "\tuse_noise = theano.shared(numpy_floatX(0.))\n",
    "\tif len(options['timeFile']) > 0: useTime = True\n",
    "\telse: useTime = False\n",
    "\n",
    "\tx = T.tensor3('x', dtype=config.floatX)\n",
    "\tt = T.matrix('t', dtype=config.floatX)\n",
    "\ty = T.tensor3('y', dtype=config.floatX)\n",
    "\tt_label = T.matrix('t_label', dtype=config.floatX)\n",
    "\tmask = T.matrix('mask', dtype=config.floatX)\n",
    "\tlengths = T.vector('lengths', dtype=config.floatX)\n",
    "\n",
    "\tn_timesteps = x.shape[0]\n",
    "\tn_samples = x.shape[1]\n",
    "\n",
    "\tif options['embFineTune']: emb = T.tanh(T.dot(x, tparams['W_emb']) + tparams['b_emb'])\n",
    "\telse: emb = T.tanh(T.dot(x, W_emb) + tparams['b_emb'])\n",
    "\tif useTime:\n",
    "\t\temb = T.concatenate([t.reshape([n_timesteps,n_samples,1]), emb], axis=2) #Adding the time element to the embedding\n",
    "\n",
    "\tinputVector = emb\n",
    "\tfor i, hiddenDimSize in enumerate(options['hiddenDimSize']):\n",
    "\t\tmemories = gru_layer(tparams, inputVector, str(i), hiddenDimSize, mask=mask)\n",
    "\t\tmemories = dropout_layer(memories, use_noise, trng, options['dropout_rate'])\n",
    "\t\tinputVector = memories\n",
    "\n",
    "\tdef softmaxStep(memory2d):\n",
    "\t\treturn T.nnet.softmax(T.dot(memory2d, tparams['W_output']) + tparams['b_output'])\n",
    "\t\n",
    "\tlogEps = options['logEps']\n",
    "\tresults, updates = theano.scan(fn=softmaxStep, sequences=[inputVector], outputs_info=None, name='softmax_layer', n_steps=n_timesteps)\n",
    "\tresults = results * mask[:,:,None]\n",
    "\tcross_entropy = -(y * T.log(results + logEps) + (1. - y) * T.log(1. - results + logEps))\n",
    "\tprediction_loss = cross_entropy.sum(axis=2).sum(axis=0) / lengths\n",
    "\n",
    "\tif options['predictTime']: \n",
    "\t\tduration = T.maximum(T.dot(inputVector, tparams['W_time']) + tparams['b_time'], 0) #ReLU\n",
    "\t\tduration = duration.reshape([n_timesteps,n_samples]) * mask\n",
    "\t\tduration_loss = 0.5 * ((duration - t_label) ** 2).sum(axis=0) / lengths\n",
    "\t\tcost = T.mean(prediction_loss) + options['tradeoff'] * T.mean(duration_loss) + options['L2_output'] * (tparams['W_output'] ** 2).sum() + options['L2_time'] * (tparams['W_time'] ** 2).sum()\n",
    "\telse: \n",
    "\t\tcost = T.mean(prediction_loss) + options['L2_output'] * (tparams['W_output'] ** 2).sum()\n",
    "\n",
    "\tif options['predictTime']: return use_noise, x, y, t, t_label, mask, lengths, cost\n",
    "\telif useTime: return use_noise, x, y, t, mask, lengths, cost\n",
    "\telse: return use_noise, x, y, mask, lengths, cost\n",
    "\n",
    "def adadelta(tparams, grads, x, y, mask, lengths, cost, options, t=None, t_label=None):\n",
    "\tzipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_grad' % k) for k, p in tparams.iteritems()]\n",
    "\trunning_up2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rup2' % k) for k, p in tparams.iteritems()]\n",
    "\trunning_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rgrad2' % k) for k, p in tparams.iteritems()]\n",
    "\n",
    "\tzgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "\trg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2)) for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "\tif options['predictTime']:\n",
    "\t\tf_grad_shared = theano.function([x, y, t, t_label, mask, lengths], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "\telif len(options['timeFile']) > 0:\n",
    "\t\tf_grad_shared = theano.function([x, y, t, mask, lengths], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "\telse:\n",
    "\t\tf_grad_shared = theano.function([x, y, mask, lengths], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "\n",
    "\tupdir = [-T.sqrt(ru2 + 1e-6) / T.sqrt(rg2 + 1e-6) * zg for zg, ru2, rg2 in zip(zipped_grads, running_up2, running_grads2)]\n",
    "\tru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2)) for ru2, ud in zip(running_up2, updir)]\n",
    "\tparam_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "\tf_update = theano.function([], [], updates=ru2up + param_up, on_unused_input='ignore', name='adadelta_f_update')\n",
    "\n",
    "\treturn f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padMatrixWithTimePrediction(seqs, labels, times, options):\n",
    "\tlengths = np.array([len(seq) for seq in seqs]) - 1\n",
    "\tn_samples = len(seqs)\n",
    "\tmaxlen = np.max(lengths)\n",
    "\tinputDimSize = options['inputDimSize']\n",
    "\tnumClass = options['numClass']\n",
    "\n",
    "\tx = np.zeros((maxlen, n_samples, inputDimSize)).astype(config.floatX)\n",
    "\ty = np.zeros((maxlen, n_samples, numClass)).astype(config.floatX)\n",
    "\tt = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tt_label = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tmask = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tfor idx, (seq,time,label) in enumerate(zip(seqs,times,labels)):\n",
    "\t\tfor xvec, subseq in zip(x[:,idx,:], seq[:-1]):\n",
    "\t\t\txvec[subseq] = 1.\n",
    "\t\tfor yvec, subseq in zip(y[:,idx,:], label[1:]):\n",
    "\t\t\tyvec[subseq] = 1.\n",
    "\t\tmask[:lengths[idx], idx] = 1.\n",
    "\t\tt[:lengths[idx], idx] = time[:-1]\n",
    "\t\tt_label[:lengths[idx], idx] = time[1:]\n",
    "\n",
    "\tlengths = np.array(lengths, dtype=config.floatX)\n",
    "\tif options['useLogTime']:\n",
    "\t\tt = np.log(t + options['logEps'])\n",
    "\t\tt_label = np.log(t_label + options['logEps'])\n",
    "\n",
    "\treturn x, y, t, t_label, mask, lengths\n",
    "\n",
    "def padMatrixWithTime(seqs, labels, times, options):\n",
    "\tlengths = np.array([len(seq) for seq in seqs]) - 1\n",
    "\tn_samples = len(seqs)\n",
    "\tmaxlen = np.max(lengths)\n",
    "\tinputDimSize = options['inputDimSize']\n",
    "\tnumClass = options['numClass']\n",
    "\n",
    "\tx = np.zeros((maxlen, n_samples, inputDimSize)).astype(config.floatX)\n",
    "\ty = np.zeros((maxlen, n_samples, numClass)).astype(config.floatX)\n",
    "\tt = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tmask = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tfor idx, (seq,time,label) in enumerate(zip(seqs,times,labels)):\n",
    "\t\tfor xvec, subseq in zip(x[:,idx,:], seq[:-1]):\n",
    "\t\t\txvec[subseq] = 1.\n",
    "\t\tfor yvec, subseq in zip(y[:,idx,:], label[1:]):\n",
    "\t\t\tyvec[subseq] = 1.\n",
    "\t\tmask[:lengths[idx], idx] = 1.\n",
    "\t\tt[:lengths[idx], idx] = time[:-1]\n",
    "\n",
    "\tlengths = np.array(lengths, dtype=config.floatX)\n",
    "\tif options['useLogTime']:\n",
    "\t\tt = np.log(t + options['logEps'])\n",
    "\n",
    "\treturn x, y, t, mask, lengths\n",
    "\n",
    "def padMatrixWithoutTime(seqs, labels, options):\n",
    "\tlengths = np.array([len(seq) for seq in seqs]) - 1\n",
    "\tn_samples = len(seqs)\n",
    "\tmaxlen = np.max(lengths)\n",
    "\tinputDimSize = options['inputDimSize']\n",
    "\tnumClass = options['numClass']\n",
    "\n",
    "\tx = np.zeros((maxlen, n_samples, inputDimSize)).astype(config.floatX)\n",
    "\ty = np.zeros((maxlen, n_samples, numClass)).astype(config.floatX)\n",
    "\tmask = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "\tfor idx, (seq,label) in enumerate(zip(seqs,labels)):\n",
    "\t\tfor xvec, subseq in zip(x[:,idx,:], seq[:-1]):\n",
    "\t\t\txvec[subseq] = 1.\n",
    "\t\tfor yvec, subseq in zip(y[:,idx,:], label[1:]):\n",
    "\t\t\tyvec[subseq] = 1.\n",
    "\t\tmask[:lengths[idx], idx] = 1.\n",
    "\n",
    "\tlengths = np.array(lengths, dtype=config.floatX)\n",
    "\n",
    "\treturn x, y, mask, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seqFile, labelFile, timeFile):\n",
    "\ttrain_set_x = pickle.load(open(seqFile+'.train', 'rb'))\n",
    "\tvalid_set_x = pickle.load(open(seqFile+'.valid', 'rb'))\n",
    "\ttest_set_x = pickle.load(open(seqFile+'.test', 'rb'))\n",
    "\ttrain_set_y = pickle.load(open(labelFile+'.train', 'rb'))\n",
    "\tvalid_set_y = pickle.load(open(labelFile+'.valid', 'rb'))\n",
    "\ttest_set_y = pickle.load(open(labelFile+'.test', 'rb'))\n",
    "\ttrain_set_t = None\n",
    "\tvalid_set_t = None\n",
    "\ttest_set_t = None\n",
    "\n",
    "\tif len(timeFile) > 0:\n",
    "\t\ttrain_set_t = pickle.load(open(timeFile+'.train', 'rb'))\n",
    "\t\tvalid_set_t = pickle.load(open(timeFile+'.valid', 'rb'))\n",
    "\t\ttest_set_t = pickle.load(open(timeFile+'.test', 'rb'))\n",
    "\n",
    "\t'''For debugging purposes\n",
    "\tsequences = np.array(pickle.load(open(seqFile, 'rb')))\n",
    "\tlabels = np.array(pickle.load(open(labelFile, 'rb')))\n",
    "\tif len(timeFile) > 0:\n",
    "\t\ttimes = np.array(pickle.load(open(timeFile, 'rb')))\n",
    "\n",
    "\tdataSize = len(labels)\n",
    "\tnp.random.seed(0)\n",
    "\tind = np.random.permutation(dataSize)\n",
    "\tnTest = int(0.15 * dataSize)\n",
    "\tnValid = int(0.10 * dataSize)\n",
    "\n",
    "\ttest_indices = ind[:nTest]\n",
    "\tvalid_indices = ind[nTest:nTest+nValid]\n",
    "\ttrain_indices = ind[nTest+nValid:]\n",
    "\n",
    "\ttrain_set_x = sequences[train_indices]\n",
    "\ttrain_set_y = labels[train_indices]\n",
    "\ttest_set_x = sequences[test_indices]\n",
    "\ttest_set_y = labels[test_indices]\n",
    "\tvalid_set_x = sequences[valid_indices]\n",
    "\tvalid_set_y = labels[valid_indices]\n",
    "\ttrain_set_t = None\n",
    "\ttest_set_t = None\n",
    "\tvalid_set_t = None\n",
    "\n",
    "\tif len(timeFile) > 0:\n",
    "\t\ttrain_set_t = times[train_indices]\n",
    "\t\ttest_set_t = times[test_indices]\n",
    "\t\tvalid_set_t = times[valid_indices]\n",
    "\t'''\n",
    "\n",
    "\tdef len_argsort(seq):\n",
    "\t\treturn sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "\ttrain_sorted_index = len_argsort(train_set_x)\n",
    "\ttrain_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "\ttrain_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "\tvalid_sorted_index = len_argsort(valid_set_x)\n",
    "\tvalid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "\tvalid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "\ttest_sorted_index = len_argsort(test_set_x)\n",
    "\ttest_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "\ttest_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "\tif len(timeFile) > 0:\n",
    "\t\ttrain_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "\t\tvalid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "\t\ttest_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "\ttrain_set = (train_set_x, train_set_y, train_set_t)\n",
    "\tvalid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "\ttest_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "\treturn train_set, valid_set, test_set\n",
    "\n",
    "def calculate_auc(test_model, dataset, options):\n",
    "\tinputDimSize = options['inputDimSize']\n",
    "\tnumClass = options['numClass']\n",
    "\tbatchSize = options['batchSize']\n",
    "\tuseTime = options['useTime']\n",
    "\tpredictTime = options['predictTime']\n",
    "\t\n",
    "\tn_batches = int(np.ceil(float(len(dataset[0])) / float(batchSize)))\n",
    "\taucSum = 0.0\n",
    "\tdataCount = 0.0\n",
    "\tfor index in xrange(n_batches):\n",
    "\t\tbatchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "\t\tbatchY = dataset[1][index*batchSize:(index+1)*batchSize]\n",
    "\t\tif predictTime:\n",
    "\t\t\tbatchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\tx, y, t, t_label, mask, lengths = padMatrixWithTimePrediction(batchX, batchY, batchT, options)\n",
    "\t\t\tauc = test_model(x, y, t, t_label, mask, lengths)\n",
    "\t\telif useTime:\n",
    "\t\t\tbatchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\tx, y, t, mask, lengths = padMatrixWithTime(batchX, batchY, batchT, options)\n",
    "\t\t\tauc = test_model(x, y, t, mask, lengths)\n",
    "\t\telse:\n",
    "\t\t\tx, y, mask, lengths = padMatrixWithoutTime(batchX, batchY, options)\n",
    "\t\t\tauc = test_model(x, y, mask, lengths)\n",
    "\t\taucSum += auc * len(batchX)\n",
    "\t\tdataCount += float(len(batchX))\n",
    "\treturn aucSum / dataCount\n",
    "\n",
    "def train_doctorAI(\n",
    "\tseqFile='seqFile.txt',\n",
    "\tinputDimSize=20000,\n",
    "\tlabelFile='labelFile.txt',\n",
    "\tnumClass=500,\n",
    "\toutFile='outFile.txt',\n",
    "\ttimeFile='timeFile.txt',\n",
    "\tpredictTime=False,\n",
    "\ttradeoff=1.0,\n",
    "\tuseLogTime=True,\n",
    "\tembFile='embFile.txt',\n",
    "\tembSize=200,\n",
    "\tembFineTune=True,\n",
    "\thiddenDimSize=[200,200],\n",
    "\tbatchSize=100,\n",
    "\tmax_epochs=10,\n",
    "\tL2_output=0.001,\n",
    "\tL2_time=0.001,\n",
    "\tdropout_rate=0.5,\n",
    "\tlogEps=1e-8,\n",
    "\tverbose=False\n",
    "):\n",
    "\toptions = locals().copy()\n",
    "\n",
    "\tif len(timeFile) > 0: useTime = True\n",
    "\telse: useTime = False\n",
    "\toptions['useTime'] = useTime\n",
    "\t\n",
    "\tprint 'Initializing the parameters ... ',\n",
    "\tparams = init_params(options)\n",
    "\ttparams = init_tparams(params, options)\n",
    "\n",
    "\tprint 'Building the model ... ',\n",
    "\tf_grad_shared = None\n",
    "\tf_update = None\n",
    "\tif predictTime and embFineTune:\n",
    "\t\tprint 'predicting duration, fine-tuning code representations'\n",
    "\t\tuse_noise, x, y, t, t_label, mask, lengths, cost =  build_model(tparams, options)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options, t, t_label)\n",
    "\telif predictTime and not embFineTune:\n",
    "\t\tprint 'predicting duration, not fine-tuning code representations'\n",
    "\t\tW_emb = theano.shared(params['W_emb'], name='W_emb')\n",
    "\t\tuse_noise, x, y, t, t_label, mask, lengths, cost =  build_model(tparams, options, W_emb)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options, t, t_label)\n",
    "\telif useTime and embFineTune:\n",
    "\t\tprint 'using duration information, fine-tuning code representations'\n",
    "\t\tuse_noise, x, y, t, mask, lengths, cost =  build_model(tparams, options)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options, t)\n",
    "\telif useTime and not embFineTune:\n",
    "\t\tprint 'using duration information, not fine-tuning code representations'\n",
    "\t\tW_emb = theano.shared(params['W_emb'], name='W_emb')\n",
    "\t\tuse_noise, x, y, t, mask, lengths, cost =  build_model(tparams, options, W_emb)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options, t)\n",
    "\telif not useTime and embFineTune:\n",
    "\t\tprint 'not using duration information, fine-tuning code representations'\n",
    "\t\tuse_noise, x, y, mask, lengths, cost =  build_model(tparams, options)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options)\n",
    "\telif useTime and not embFineTune:\n",
    "\t\tprint 'not using duration information, not fine-tuning code representations'\n",
    "\t\tW_emb = theano.shared(params['W_emb'], name='W_emb')\n",
    "\t\tuse_noise, x, y, mask, lengths, cost =  build_model(tparams, options, W_emb)\n",
    "\t\tgrads = T.grad(cost, wrt=tparams.values())\n",
    "\t\tf_grad_shared, f_update = adadelta(tparams, grads, x, y, mask, lengths, cost, options)\n",
    "\n",
    "\tprint 'Loading data ... ',\n",
    "\ttrainSet, validSet, testSet = load_data(seqFile, labelFile, timeFile)\n",
    "\tn_batches = int(np.ceil(float(len(trainSet[0])) / float(batchSize)))\n",
    "\tprint 'done'\n",
    "\n",
    "\tif predictTime: test_model = theano.function(inputs=[x, y, t, t_label, mask, lengths], outputs=cost, name='test_model')\n",
    "\telif useTime: test_model = theano.function(inputs=[x, y, t, mask, lengths], outputs=cost, name='test_model')\n",
    "\telse: test_model = theano.function(inputs=[x, y, mask, lengths], outputs=cost, name='test_model')\n",
    "\n",
    "\tbestValidCrossEntropy = 1e20\n",
    "\tbestValidEpoch = 0\n",
    "\ttestCrossEntropy = 0.0\n",
    "\tprint 'Optimization start !!'\n",
    "\tfor epoch in xrange(max_epochs):\n",
    "\t\titeration = 0\n",
    "\t\tcostVector = []\n",
    "\t\tfor index in random.sample(range(n_batches), n_batches):\n",
    "\t\t\tuse_noise.set_value(1.)\n",
    "\t\t\tbatchX = trainSet[0][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\tbatchY = trainSet[1][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\tif predictTime:\n",
    "\t\t\t\tbatchT = trainSet[2][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\t\tx, y, t, t_label, mask, lengths = padMatrixWithTimePrediction(batchX, batchY, batchT, options)\n",
    "\t\t\t\tcost = f_grad_shared(x, y, t, t_label, mask, lengths)\n",
    "\t\t\telif useTime:\n",
    "\t\t\t\tbatchT = trainSet[2][index*batchSize:(index+1)*batchSize]\n",
    "\t\t\t\tx, y, t, mask, lengths = padMatrixWithTime(batchX, batchY, batchT, options)\n",
    "\t\t\t\tcost = f_grad_shared(x, y, t, mask, lengths)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx, y, mask, lengths = padMatrixWithoutTime(batchX, batchY, options)\n",
    "\t\t\t\tcost = f_grad_shared(x, y, mask, lengths)\n",
    "\t\t\tcostVector.append(cost)\n",
    "\t\t\tf_update()\n",
    "\t\t\tif (iteration % 10 == 0) and verbose: print 'epoch:%d, iteration:%d/%d, cost:%f' % (epoch, iteration, n_batches, cost)\n",
    "\t\t\titeration += 1\n",
    "\n",
    "\t\tprint 'epoch:%d, mean_cost:%f' % (epoch, np.mean(costVector))\n",
    "\t\tuse_noise.set_value(0.)\n",
    "\t\tvalidAuc = calculate_auc(test_model, validSet, options)\n",
    "\t\tprint 'Validation cross entropy:%f at epoch:%d' % (validAuc, epoch)\n",
    "\t\tif validAuc < bestValidCrossEntropy: \n",
    "\t\t\tbestValidCrossEntropy = validAuc\n",
    "\t\t\tbestValidEpoch = epoch\n",
    "\t\t\tbestParams = unzip(tparams)\n",
    "\t\t\ttestCrossEntropy = calculate_auc(test_model, testSet, options)\n",
    "\t\t\tprint 'Test cross entropy:%f at epoch:%d' % (testCrossEntropy, epoch)\n",
    "\t\t\ttempParams = unzip(tparams)\n",
    "\t\t\tnp.savez_compressed(outFile + '.' + str(epoch), **tempParams)\n",
    "\tprint 'The best valid cross entropy:%f at epoch:%d' % (bestValidCrossEntropy, bestValidEpoch)\n",
    "\tprint 'The test cross entropy: %f' % testCrossEntropy\n",
    "\t\n",
    "def parse_arguments(parser):\n",
    "\tparser.add_argument('seq_file', type=str, metavar='<visit_file>', help='The path to the Pickled file containing visit information of patients')\n",
    "\tparser.add_argument('n_input_codes', type=int, metavar='<n_input_codes>', help='The number of unique input medical codes')\n",
    "\tparser.add_argument('label_file', type=str, metavar='<label_file>', help='The path to the Pickled file containing label information of patients')\n",
    "\tparser.add_argument('n_output_codes', type=int, metavar='<n_output_codes>', help='The number of unique label medical codes')\n",
    "\tparser.add_argument('out_file', metavar='out_file', help='The path to the output models. The models will be saved after every epoch')\n",
    "\tparser.add_argument('--time_file', type=str, default='', help='The path to the Pickled file containing durations between visits of patients. If you are not using duration information, do not use this option')\n",
    "\tparser.add_argument('--predict_time', type=int, default=0, choices=[0,1], help='Use this option if you want the GRU to also predict the time duration until the next visit (0 for false, 1 for true) (default value: 0)')\n",
    "\tparser.add_argument('--tradeoff', type=float, default=1.0, help='Tradeoff variable for balancing the two loss functions: code prediction function and duration prediction function (default value: 1.0)')\n",
    "\tparser.add_argument('--use_log_time', type=int, default=1, choices=[0,1], help='Use logarithm of time duration to dampen the impact of the outliers (0 for false, 1 for true) (default value: 1)')\n",
    "\tparser.add_argument('--embed_file', type=str, default='', help='The path to the Pickled file containing the representation vectors of medical codes. If you are not using medical code representations, do not use this option')\n",
    "\tparser.add_argument('--embed_size', type=int, default=200, help='The size of the visit embedding before passing it to the GRU layers. If you are not providing your own medical code vectors, you must specify this value (default value: 200)')\n",
    "\tparser.add_argument('--embed_finetune', type=int, default=1, choices=[0,1], help='If you are using randomly initialized code representations, always use this option. If you are using an external medical code representations, and you want to fine-tune them as you train the GRU, use this option as well. (0 for false, 1 for true) (default value: 1)')\n",
    "\tparser.add_argument('--hidden_dim_size', type=str, default='[200,200]', help='The size of the hidden layers of the GRU. This is a string argument. For example, [500,400] means you are using a two-layer GRU where the lower layer uses a 500-dimensional hidden layer, and the upper layer uses a 400-dimensional hidden layer. (default value: [200,200])')\n",
    "\tparser.add_argument('--batch_size', type=int, default=100, help='The size of a single mini-batch (default value: 100)')\n",
    "\tparser.add_argument('--n_epochs', type=int, default=10, help='The number of training epochs (default value: 10)')\n",
    "\tparser.add_argument('--L2_softmax', type=float, default=0.001, help='L2 regularization for the softmax function (default value: 0.001)')\n",
    "\tparser.add_argument('--L2_time', type=float, default=0.001, help='L2 regularization for the linear regression (default value: 0.001)')\n",
    "\tparser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate between GRU hidden layers, and between the final hidden layer and the softmax layer (default value: 0.5)')\n",
    "\tparser.add_argument('--log_eps', type=float, default=1e-8, help='A small value to prevent log(0) (default value: 1e-8)')\n",
    "\tparser.add_argument('--verbose', action='store_true', help='Print output after every 10 mini-batches (default false)')\n",
    "\targs = parser.parse_args()\n",
    "\treturn args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\targs = parse_arguments(parser)\n",
    "\thiddenDimSize = [int(strDim) for strDim in args.hidden_dim_size[1:-1].split(',')]\n",
    "\n",
    "\tif args.predict_time and args.time_file == '':\n",
    "\t\tprint 'Cannot predict time duration without time file'\n",
    "\t\tsys.exit()\n",
    "\n",
    "\ttrain_doctorAI(\n",
    "\t\tseqFile=args.seq_file, \n",
    "\t\tinputDimSize=args.n_input_codes, \n",
    "\t\tlabelFile=args.label_file, \n",
    "\t\tnumClass=args.n_output_codes, \n",
    "\t\toutFile=args.out_file, \n",
    "\t\ttimeFile=args.time_file, \n",
    "\t\tpredictTime=args.predict_time,\n",
    "\t\ttradeoff=args.tradeoff,\n",
    "\t\tuseLogTime=args.use_log_time,\n",
    "\t\tembFile=args.embed_file, \n",
    "\t\tembSize=args.embed_size, \n",
    "\t\tembFineTune=args.embed_finetune, \n",
    "\t\thiddenDimSize=hiddenDimSize,\n",
    "\t\tbatchSize=args.batch_size, \n",
    "\t\tmax_epochs=args.n_epochs, \n",
    "\t\tL2_output=args.L2_softmax, \n",
    "\t\tL2_time=args.L2_time, \n",
    "\t\tdropout_rate=args.dropout_rate, \n",
    "\t\tlogEps=args.log_eps, \n",
    "\t\tverbose=args.verbose\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
