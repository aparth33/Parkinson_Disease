{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    " \n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from sklearn.cross_validation import KFold # use for cross validation\n",
    "from sklearn.preprocessing import StandardScaler # for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline # pipeline making\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "## for Deep-learing:\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Keras tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeDistributed Wraper Layer\n",
    "This wrapper applies a layer to every temporal slice of an input.\n",
    "The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.\n",
    "Consider a batch of 32 samples, where each sample is a sequence of 10 vectors of 16 dimensions. The batch input shape of the layer is then (32, 10, 16), and the input_shape, not including the samples dimension, is  (10, 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the first layer in a model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n",
    "# now model.output_shape == (None, 10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeSeriesGenerator\n",
    "Utility class for generating batches of temporal data.\n",
    "This class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[i] for i in range(50)])\n",
    "targets = np.array([[i] for i in range(50)])\n",
    "\n",
    "data_gen = TimeseriesGenerator(data, targets,\n",
    "                               length=10, sampling_rate=2,\n",
    "                               batch_size=2)\n",
    "assert len(data_gen) == 20\n",
    "\n",
    "batch_0 = data_gen[0]\n",
    "x, y = batch_0\n",
    "assert np.array_equal(x,\n",
    "                      np.array([[[0], [2], [4], [6], [8]],\n",
    "                                [[1], [3], [5], [7], [9]]]))\n",
    "assert np.array_equal(y,\n",
    "                      np.array([[10], [11]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Layers\n",
    "https://keras.io/layers/recurrent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TensorFlow optimisers (Irwan optimiser)\n",
    "keras.optimizers.TFOptimizer(optimizer)\n",
    "\n",
    "#### Other things that could be used: \n",
    "https://github.com/keras-team/keras/issues/5348"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeDistributed usage for many-to-many sequence prediction\n",
    "source: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# prepare sequence \n",
    "# you need to have a 3D-sequence with (number of samples, number of timesteps, number of features)\n",
    "length = 5\n",
    "seq = array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1)\n",
    "y = seq.reshape(1, length, 1)\n",
    "\n",
    "# define LSTM configuration\n",
    "n_neurons = length\n",
    "n_batch = 1 \n",
    "n_epoch = 1000\n",
    "\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1))) # 1 is the number of features you want to return. \n",
    "#TimeDistributed applies a layer (can be dense, LTSM, conv...) to every output timestep. \n",
    "#To be used for many-to-many sequence prediction and for one-to-many sequence prediction. \n",
    "#see post here: https://github.com/keras-team/keras/issues/1029\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=2) \n",
    "#if you have multiple time-series, you set the batch_size to the number of your time-series. \n",
    "\n",
    "# evaluate\n",
    "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
    "for value in result[0,:,0]:\n",
    "\tprint('%.1f' % value)\n",
    "    \n",
    "#source: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this post as well: https://groups.google.com/forum/#!topic/keras-users/9GsDwkSdqBg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation steps: \n",
    "0. use diagfeat_EADL exported csv file. \n",
    "1. do some padding so that you have sequences of same length\n",
    "1.bis: do not forget normaization!\n",
    "2. Have a clean training 3D-array of shape (number_of_patients,number_of_visits,number_of_features)\n",
    "3. Have a clean target 3D-array of shape (number_of_patients,timesteps_to_predict,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to try\n",
    "1. LSTM/GRUs on subset of features\n",
    "2. seq2seq model (multivariate with only the target)\n",
    "3. LSTM-MFCN: see github repo and paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    '''transform the data in a 3D-array shape with padding'''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(): \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model(): \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
